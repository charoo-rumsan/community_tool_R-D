{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/charoo-rumsan/community_tool_research/blob/main/classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import tracemalloc\n",
        "import re"
      ],
      "metadata": {
        "id": "nFbmnGYLb19X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "metadata": {
        "id": "LEIfoFoYSA7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb"
      ],
      "metadata": {
        "id": "s83bmnA8SDrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Simulate datasets\n",
        "df1 = pd.DataFrame({\n",
        "\"Contact Info\": [\"john.doe@company.com\", \"jane.smith@mail.com\", \"michael.lee@hr.com\", \"emma.brown@xyz.org\", \"david.johnson@work.io\"],\n",
        "\"Phone Number\": [\"(555) 123-4567\", \"555-987-6543\", \"212-345-7890\", \"+1 646-555-1212\", \"(310) 789-4321\"],\n",
        "\"Location\": [\"123 Main St, New York, NY\", \"45 Wall Street, New York, NY\", \"67 Park Ave, Boston, MA\", \"89 Oak Road, Chicago, IL\", \"12 Sunset Blvd, Los Angeles\"],\n",
        "\"Full Name\": [\"John Doe\", \"Jane Smith\", \"Michael Lee\", \"Emma Brown\", \"David Johnson\"],\n",
        "\"Emp_ID\": [\"E001\", \"E002\", \"E003\", \"E004\", \"E005\"]\n",
        "})\n",
        "\n",
        "\n",
        "df2 = pd.DataFrame({\n",
        "\"Email\": [\"sarah.connor@work.com\", \"tom.hardy@jobs.org\", \"alice.wong@mail.net\", \"robert.white@staff.io\", \"linda.green@corp.org\"],\n",
        "\"Mobile\": [\"202-333-4567\", \"+44 7700 900123\", \"(415) 222-9876\", \"646-444-1212\", \"555-678-9999\"],\n",
        "\"Address\": [\"100 King St, Washington, DC\", \"22 Queen Rd, London, UK\", \"78 Market St, San Francisco\", \"15 Pine Lane, Boston, MA\", \"200 Broadway, New York, NY\"],\n",
        "\"Employee Name\": [\"Sarah Connor\", \"Tom Hardy\", \"Alice Wong\", \"Robert White\", \"Linda Green\"],\n",
        "\"WorkerID\": [\"W101\", \"W102\", \"W103\", \"W104\", \"W105\"]\n",
        "})"
      ],
      "metadata": {
        "id": "Psj8wGozSIUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard labels\n",
        "labels = {\n",
        "\"Contact Info\": \"email_address\",\n",
        "\"Email\": \"email_address\",\n",
        "\"Phone Number\": \"phone_number\",\n",
        "\"Mobile\": \"phone_number\",\n",
        "\"Location\": \"address\",\n",
        "\"Address\": \"address\",\n",
        "\"Full Name\": \"employee_name\",\n",
        "\"Employee Name\": \"employee_name\",\n",
        "\"Emp_ID\": \"employee_id\",\n",
        "\"WorkerID\": \"employee_id\"\n",
        "}"
      ],
      "metadata": {
        "id": "1HjAlOfHSM8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Feature Extraction\n",
        "def extract_features(col_values):\n",
        "    sample = col_values.dropna().astype(str).values[:50] # sample max 50 rows\n",
        "    joined = \" \".join(sample)\n",
        "    features = {\n",
        "        \"has_at\": int(\"@\" in joined),\n",
        "        \"digit_ratio\": sum(c.isdigit() for c in joined) / max(1, len(joined)),\n",
        "        \"avg_len\": np.mean([len(v) for v in sample]),\n",
        "        \"has_commas\": int(\",\" in joined),\n",
        "        \"has_plus\": int(\"+\" in joined),\n",
        "        \"has_spaces\": int(\" \" in joined),\n",
        "    }\n",
        "    return features"
      ],
      "metadata": {
        "id": "_eWCUCtkSTfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build training data\n",
        "X, y = [], []\n",
        "for df in [df1, df2]:\n",
        "    for col in df.columns:\n",
        "        feats = extract_features(df[col])\n",
        "        X.append(list(feats.values()))\n",
        "        y.append(labels[col])\n",
        "\n",
        "feature_names = list(extract_features(df1[\"Contact Info\"]).keys())"
      ],
      "metadata": {
        "id": "YEYPO-LfSZxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "y_enc = le.fit_transform(y)"
      ],
      "metadata": {
        "id": "9ZHzy80JSmbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Train/Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_enc, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "waMmFMuWSrgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Benchmark Models\n",
        "\n",
        "models = {\n",
        "\"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "\"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "\"XGBoost\": xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
        "}\n",
        "\n",
        "\n",
        "results = []\n",
        "\n",
        "\n",
        "for name, model in models.items():\n",
        "    tracemalloc.start()\n",
        "    start_time = time.time()\n",
        "\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "    current, peak = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Get the unique labels present in the test set\n",
        "    unique_labels_test = np.unique(y_test)\n",
        "    # Get the corresponding target names using the label encoder\n",
        "    target_names_test = le.classes_[unique_labels_test]\n",
        "\n",
        "\n",
        "    results.append({\n",
        "    \"Model\": name,\n",
        "    \"Accuracy\": acc,\n",
        "    \"Training Time (s)\": round(end_time - start_time, 4),\n",
        "    \"Current Memory (MB)\": round(current / 10**6, 2),\n",
        "    \"Peak Memory (MB)\": round(peak / 10**6, 2)\n",
        "    })\n",
        "\n",
        "\n",
        "    print(\"\\n===\", name, \"===\")\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"Training Time: {end_time - start_time:.4f} seconds\")\n",
        "    print(f\"Current Memory Usage: {current / 10**6:.2f} MB\")\n",
        "    print(f\"Peak Memory Usage: {peak / 10**6:.2f} MB\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, labels=unique_labels_test, target_names=target_names_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAlvOm80Svc6",
        "outputId": "c4736985-cc23-498a-d2a2-9b28b2263deb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Logistic Regression ===\n",
            "Accuracy: 1.0000\n",
            "Training Time: 0.1559 seconds\n",
            "Current Memory Usage: 0.02 MB\n",
            "Peak Memory Usage: 0.06 MB\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "email_address       1.00      1.00      1.00         1\n",
            "employee_name       1.00      1.00      1.00         1\n",
            " phone_number       1.00      1.00      1.00         1\n",
            "\n",
            "     accuracy                           1.00         3\n",
            "    macro avg       1.00      1.00      1.00         3\n",
            " weighted avg       1.00      1.00      1.00         3\n",
            "\n",
            "\n",
            "=== Random Forest ===\n",
            "Accuracy: 1.0000\n",
            "Training Time: 1.9918 seconds\n",
            "Current Memory Usage: 0.14 MB\n",
            "Peak Memory Usage: 0.16 MB\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "email_address       1.00      1.00      1.00         1\n",
            "employee_name       1.00      1.00      1.00         1\n",
            " phone_number       1.00      1.00      1.00         1\n",
            "\n",
            "     accuracy                           1.00         3\n",
            "    macro avg       1.00      1.00      1.00         3\n",
            " weighted avg       1.00      1.00      1.00         3\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [04:40:37] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== XGBoost ===\n",
            "Accuracy: 0.0000\n",
            "Training Time: 1.7080 seconds\n",
            "Current Memory Usage: 0.02 MB\n",
            "Peak Memory Usage: 0.03 MB\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "email_address       0.00      0.00      0.00       1.0\n",
            "employee_name       0.00      0.00      0.00       1.0\n",
            " phone_number       0.00      0.00      0.00       1.0\n",
            "\n",
            "    micro avg       0.00      0.00      0.00       3.0\n",
            "    macro avg       0.00      0.00      0.00       3.0\n",
            " weighted avg       0.00      0.00      0.00       3.0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Summary Table\n",
        "\n",
        "summary_df = pd.DataFrame(results)\n",
        "print(\"\\n Benchmark Results\")\n",
        "print(summary_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgTfdAh3TcaT",
        "outputId": "85d4cde5-e32b-422e-d6c7-898351d7e392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Benchmark Results\n",
            "                 Model  Accuracy  Training Time (s)  Current Memory (MB)  \\\n",
            "0  Logistic Regression       1.0             0.1559                 0.02   \n",
            "1        Random Forest       1.0             1.9918                 0.14   \n",
            "2              XGBoost       0.0             1.7080                 0.02   \n",
            "\n",
            "   Peak Memory (MB)  \n",
            "0              0.06  \n",
            "1              0.16  \n",
            "2              0.03  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement 5 approaches to classify HR dataset fields.\n",
        " Approaches:\n",
        " 1. Classical ML (Logistic Regression, Random Forest, XGBoost)\n",
        " 2. Regex + ML Hybrid\n",
        " 3. Unsupervised Clustering (KMeans)\n",
        " 4. AutoML (TPOT / Auto-sklearn)\n",
        " 5. Deep Learning (DistilBERT"
      ],
      "metadata": {
        "id": "qH9qnI8kXGUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost -q"
      ],
      "metadata": {
        "id": "aytKSFZVTMJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7e9481e",
        "outputId": "4da6724f-3877-4a50-84f6-674b166ac5f9"
      },
      "source": [
        "!pip install tpot auto-sklearn transformers torch -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Exception:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "                      ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 95, in resolve\n",
            "    result = self._result = resolver.resolve(\n",
            "                            ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 546, in resolve\n",
            "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 427, in resolve\n",
            "    failure_causes = self._attempt_to_pin_criterion(name)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 239, in _attempt_to_pin_criterion\n",
            "    criteria = self._get_updated_criteria(candidate)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 230, in _get_updated_criteria\n",
            "    self._add_to_criteria(criteria, requirement, parent=candidate)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 173, in _add_to_criteria\n",
            "    if not criterion.candidates:\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/resolvelib/structs.py\", line 156, in __bool__\n",
            "    return bool(self._sequence)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 174, in __bool__\n",
            "    return any(self)\n",
            "           ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 162, in <genexpr>\n",
            "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
            "                       ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 53, in _iter_built\n",
            "    candidate = func()\n",
            "                ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 185, in _make_candidate_from_link\n",
            "    base: Optional[BaseCandidate] = self._make_base_candidate_from_link(\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 231, in _make_base_candidate_from_link\n",
            "    self._link_candidate_cache[link] = LinkCandidate(\n",
            "                                       ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 303, in __init__\n",
            "    super().__init__(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 158, in __init__\n",
            "    self.dist = self._prepare()\n",
            "                ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 235, in _prepare\n",
            "    dist = self._prepare_distribution()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 314, in _prepare_distribution\n",
            "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/prepare.py\", line 527, in prepare_linked_requirement\n",
            "    return self._prepare_linked_requirement(req, parallel_builds)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/prepare.py\", line 642, in _prepare_linked_requirement\n",
            "    dist = _get_prepared_distribution(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/prepare.py\", line 72, in _get_prepared_distribution\n",
            "    abstract_dist.prepare_distribution_metadata(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/distributions/sdist.py\", line 56, in prepare_distribution_metadata\n",
            "    self._install_build_reqs(finder)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/distributions/sdist.py\", line 126, in _install_build_reqs\n",
            "    build_reqs = self._get_build_requires_wheel()\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/distributions/sdist.py\", line 103, in _get_build_requires_wheel\n",
            "    return backend.get_requires_for_build_wheel()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/utils/misc.py\", line 709, in get_requires_for_build_wheel\n",
            "    return super().get_requires_for_build_wheel(config_settings=cs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/pyproject_hooks/_impl.py\", line 166, in get_requires_for_build_wheel\n",
            "    return self._call_hook('get_requires_for_build_wheel', {\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/pyproject_hooks/_impl.py\", line 321, in _call_hook\n",
            "    raise BackendUnavailable(data.get('traceback', ''))\n",
            "pip._vendor.pyproject_hooks._impl.BackendUnavailable: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 77, in _build_backend\n",
            "    obj = import_module(mod_path)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/tmp/pip-build-env-c29k024u/overlay/local/lib/python3.12/dist-packages/setuptools/__init__.py\", line 16, in <module>\n",
            "    import setuptools.version\n",
            "  File \"/tmp/pip-build-env-c29k024u/overlay/local/lib/python3.12/dist-packages/setuptools/version.py\", line 1, in <module>\n",
            "    import pkg_resources\n",
            "  File \"/tmp/pip-build-env-c29k024u/overlay/local/lib/python3.12/dist-packages/pkg_resources/__init__.py\", line 73, in <module>\n",
            "    from pkg_resources.extern import appdirs\n",
            "ImportError: cannot import name 'appdirs' from 'pkg_resources.extern' (/tmp/pip-build-env-c29k024u/overlay/local/lib/python3.12/dist-packages/pkg_resources/extern/__init__.py)\n",
            "\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import time\n",
        "import tracemalloc\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, adjusted_rand_score\n",
        "\n",
        "\n",
        "import xgboost as xgb\n",
        "from tpot import TPOTClassifier\n",
        "import autosklearn.classification\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "vCV5Vd9yS0k9",
        "outputId": "60f9c7e6-fdc2-4983-900d-d5f13ca10cfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tpot'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1097300130.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtpot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTPOTClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautosklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistilBertTokenizerFast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistilBertForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tpot'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Simulate datasets\n",
        "df1 = pd.DataFrame({\n",
        "\"Contact Info\": [\"john.doe@company.com\", \"jane.smith@mail.com\", \"michael.lee@hr.com\", \"emma.brown@xyz.org\", \"david.johnson@work.io\"],\n",
        "\"Phone Number\": [\"(555) 123-4567\", \"555-987-6543\", \"212-345-7890\", \"+1 646-555-1212\", \"(310) 789-4321\"],\n",
        "\"Location\": [\"123 Main St, New York, NY\", \"45 Wall Street, New York, NY\", \"67 Park Ave, Boston, MA\", \"89 Oak Road, Chicago, IL\", \"12 Sunset Blvd, Los Angeles\"],\n",
        "\"Full Name\": [\"John Doe\", \"Jane Smith\", \"Michael Lee\", \"Emma Brown\", \"David Johnson\"],\n",
        "\"Emp_ID\": [\"E001\", \"E002\", \"E003\", \"E004\", \"E005\"]\n",
        "})\n",
        "\n",
        "\n",
        "df2 = pd.DataFrame({\n",
        "\"Email\": [\"sarah.connor@work.com\", \"tom.hardy@jobs.org\", \"alice.wong@mail.net\", \"robert.white@staff.io\", \"linda.green@corp.org\"],\n",
        "\"Mobile\": [\"202-333-4567\", \"+44 7700 900123\", \"(415) 222-9876\", \"646-444-1212\", \"555-678-9999\"],\n",
        "\"Address\": [\"100 King St, Washington, DC\", \"22 Queen Rd, London, UK\", \"78 Market St, San Francisco\", \"15 Pine Lane, Boston, MA\", \"200 Broadway, New York, NY\"],\n",
        "\"Employee Name\": [\"Sarah Connor\", \"Tom Hardy\", \"Alice Wong\", \"Robert White\", \"Linda Green\"],\n",
        "\"WorkerID\": [\"W101\", \"W102\", \"W103\", \"W104\", \"W105\"]\n",
        "})"
      ],
      "metadata": {
        "id": "jQJJtUl6Xlk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Labels\n",
        "labels = {\n",
        "\"Contact Info\": \"email_address\",\n",
        "\"Email\": \"email_address\",\n",
        "\"Phone Number\": \"phone_number\",\n",
        "\"Mobile\": \"phone_number\",\n",
        "\"Location\": \"address\",\n",
        "\"Address\": \"address\",\n",
        "\"Full Name\": \"employee_name\",\n",
        "\"Employee Name\": \"employee_name\",\n",
        "\"Emp_ID\": \"employee_id\",\n",
        "\"WorkerID\": \"employee_id\"\n",
        "}"
      ],
      "metadata": {
        "id": "u9k3avTyXprT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Feature Extraction\n",
        "def extract_features(col_values):\n",
        "    sample = col_values.dropna().astype(str).values[:50]\n",
        "    joined = \" \".join(sample)\n",
        "    return {\n",
        "        \"has_at\": int(\"@\" in joined),\n",
        "        \"digit_ratio\": sum(c.isdigit() for c in joined) / max(1, len(joined)),\n",
        "        \"avg_len\": np.mean([len(v) for v in sample]),\n",
        "        \"has_commas\": int(\",\" in joined),\n",
        "        \"has_plus\": int(\"+\" in joined),\n",
        "        \"has_spaces\": int(\" \" in joined),\n",
        "    }\n",
        "\n",
        "\n",
        "def extract_regex_features(col_values):\n",
        "    sample = col_values.dropna().astype(str).values[:50]\n",
        "    joined = \" \".join(sample)\n",
        "    return {\n",
        "        \"email_pattern\": int(bool(re.search(r\"[\\w._%+-]+@[\\w.-]+\", joined))),\n",
        "        \"phone_pattern\": int(bool(re.search(r\"\\d{3}[- )]\\d{3}[- ]\\d{4}\", joined))),\n",
        "        \"address_pattern\": int(\",\" in joined and any(ch.isdigit() for ch in joined)),\n",
        "        \"name_pattern\": int(all(w.istitle() for w in sample[0].split())),\n",
        "        \"id_pattern\": int(bool(re.search(r\"[A-Z]\\d+\", joined)))\n",
        "    }"
      ],
      "metadata": {
        "id": "84kRvLllXszi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Build Data\n",
        "\n",
        "X, y, X_regex = [], [], []\n",
        "for df in [df1, df2]:\n",
        "    for col in df.columns:\n",
        "        feats = extract_features(df[col])\n",
        "        regex_feats = extract_regex_features(df[col])\n",
        "        X.append(list(feats.values()))\n",
        "        X_regex.append(list(regex_feats.values()))\n",
        "        y.append(labels[col])\n",
        "\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "y_enc = le.fit_transform(y)"
      ],
      "metadata": {
        "id": "EhWY4RuHXyoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Define Benchmark Function\n",
        "\n",
        "def benchmark_model(name, model, X_train, X_test, y_train, y_test):\n",
        "    tracemalloc.start()\n",
        "    start = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    end = time.time()\n",
        "    current, peak = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    return {\n",
        "    \"Model\": name,\n",
        "    \"Accuracy\": acc,\n",
        "    \"Training Time (s)\": round(end - start, 4),\n",
        "    \"Current Memory (MB)\": round(current / 1e6, 2),\n",
        "    \"Peak Memory (MB)\": round(peak / 1e6, 2)\n",
        "    }"
      ],
      "metadata": {
        "id": "P4vDl286YEYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Approach 1: Classical ML\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_enc, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "results = []\n",
        "results.append(benchmark_model(\"Logistic Regression\", LogisticRegression(max_iter=1000), X_train, X_test, y_train, y_test))\n",
        "results.append(benchmark_model(\"Random Forest\", RandomForestClassifier(random_state=42), X_train, X_test, y_train, y_test))\n",
        "results.append(benchmark_model(\"XGBoost\", xgb.XGBClassifier(eval_metric='mlogloss'), X_train, X_test, y_train, y_test))"
      ],
      "metadata": {
        "id": "rwG0tzlLYH9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Approach 2: Regex + ML Hybrid\n",
        "\n",
        "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_regex, y_enc, test_size=0.3, random_state=42)\n",
        "results.append(benchmark_model(\"Regex+LogReg\", LogisticRegression(max_iter=1000), X_train_r, X_test_r, y_train_r, y_test_r))"
      ],
      "metadata": {
        "id": "FqUi8ughYNWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Approach 3: Unsupervised Clustering\n",
        "\n",
        "kmeans = KMeans(n_clusters=len(set(y_enc)), random_state=42)\n",
        "kmeans.fit(X)\n",
        "y_pred_kmeans = kmeans.labels_\n",
        "results.append({\n",
        "\"Model\": \"KMeans Clustering\",\n",
        "\"Accuracy\": adjusted_rand_score(y_enc, y_pred_kmeans),\n",
        "\"Training Time (s)\": None,\n",
        "\"Current Memory (MB)\": None,\n",
        "\"Peak Memory (MB)\": None\n",
        "})"
      ],
      "metadata": {
        "id": "y_TNtXx6YSNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Approach 4: AutoML (TPOT demo, small generations)\n",
        "\n",
        "tpot = TPOTClassifier(generations=2, population_size=5, verbosity=0)\n",
        "results.append(benchmark_model(\"TPOT AutoML\", tpot, X_train, X_test, y_train, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "FZZiUFgeYXBl",
        "outputId": "eda4d25d-d7f1-4c36-fe71-078263c14290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'TPOTClassifier' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-724515293.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Approach 4: AutoML (TPOT demo, small generations)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtpot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTPOTClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpopulation_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbosity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbenchmark_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TPOT AutoML\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TPOTClassifier' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Approach 5: Deep Learning (DistilBERT)\n",
        "# Prepare text data: column name + sample values\n",
        "\n",
        "texts = []\n",
        "for df in [df1, df2]:\n",
        "    for col in df.columns:\n",
        "        sample = \" \".join(df[col].astype(str).values[:3])\n",
        "        texts.append(col + \" \" + sample)\n",
        "\n",
        "\n",
        "X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(texts, y_enc, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "qjLT9C-nYbG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
        "train_encodings = tokenizer(X_train_t, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(X_test_t, truncation=True, padding=True)\n",
        "\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "        return {k: torch.tensor(v[idx]) for k, v in self.encodings.items()} | {\"labels\": torch.tensor(self.labels[idx])}\n",
        "\n",
        "\n",
        "train_dataset = Dataset(train_encodings, y_train_t)\n",
        "test_dataset = Dataset(test_encodings, y_test_t)\n",
        "\n",
        "\n",
        "model_dl = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=len(le.classes_))\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"./results\", num_train_epochs=1, per_device_train_batch_size=2, logging_dir=\"./logs\", logging_steps=5)\n",
        "trainer = Trainer(model=model_dl, args=training_args, train_dataset=train_dataset, eval_dataset=test_dataset)\n",
        "\n",
        "\n",
        "tracemalloc.start()\n",
        "start = time.time()\n",
        "trainer.train()\n",
        "end = time.time()\n",
        "current, peak = tracemalloc.get_traced_memory()\n",
        "tracemalloc.stop()\n",
        "\n",
        "\n",
        "preds = trainer.predict(test_dataset)\n",
        "y_pred_dl = np.argmax(preds.predictions, axis=1)\n",
        "acc = accuracy_score(y_test_t, y_pred_dl)\n",
        "\n",
        "\n",
        "results.append({\n",
        "    \"Model\": \"DistilBERT\",\n",
        "    \"Accuracy\": acc,\n",
        "    \"Training Time (s)\": round(end - start, 4),\n",
        "    \"Current Memory (MB)\": round(current / 1e6, 2),\n",
        "    \"Peak Memory (MB)\": round(peak / 1e6, 2)\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "U5Grri7xYfdc",
        "outputId": "a31efa13-c68b-42be-f45f-6c34ea821640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'DistilBertTokenizerFast' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2916950698.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"distilbert-base-uncased\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDistilBertTokenizerFast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrain_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DistilBertTokenizerFast' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Summary\n",
        "\n",
        "summary_df = pd.DataFrame(results)\n",
        "print(\"Benchmark Results\")\n",
        "print(summary_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWdqVXDyYnb7",
        "outputId": "8413eb65-3588-4ecd-914d-33e293eb1964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Benchmark Results\n",
            "                 Model  Accuracy  Training Time (s)  Current Memory (MB)  \\\n",
            "0  Logistic Regression  1.000000             0.1578                 0.01   \n",
            "1        Random Forest  1.000000             1.4263                 0.14   \n",
            "2              XGBoost  0.000000             0.0539                 0.01   \n",
            "3         Regex+LogReg  0.666667             0.0100                 0.01   \n",
            "4    KMeans Clustering  1.000000                NaN                  NaN   \n",
            "\n",
            "   Peak Memory (MB)  \n",
            "0              0.05  \n",
            "1              0.17  \n",
            "2              0.01  \n",
            "3              0.05  \n",
            "4               NaN  \n"
          ]
        }
      ]
    }
  ]
}